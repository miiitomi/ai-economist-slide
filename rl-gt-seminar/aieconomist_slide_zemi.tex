\documentclass[unicode,aspectratio=169,11pt]{beamer}
\usepackage{amsmath, amssymb, amsthm, color, latexsym, mathrsfs, bm}
\usefonttheme{professionalfonts}
\usepackage{luatexja}
\usepackage[ipaex]{luatexja-preset}
\renewcommand{\kanjifamilydefault}{\gtdefault}

\usetheme[
  sectionpage=none,
  numbering=fraction,
  block=fill
  ]{metropolis}

\title{
    The AI economist:
    Improving Equality and Productivity with AI-Driven Tax Policies
}
\subtitle{Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C. Parkes, and Richard Socher, 2020, mimeo.}
\author{Presenter: Yoji Tomita}
\date{RL-GTゼミ June 17, 2021}

\begin{document}

\maketitle

\begin{frame}{Table of Contents}
    \tableofcontents
\end{frame}

\section{1. Introduction}
\begin{frame}{1. Introduction}
    \begin{itemize}
        \item イントロダクション
    \end{itemize}
\end{frame}

\section{2. Economic Simulations: Learning in Gather-and-Build Games}

\begin{frame}{2. Economic Simulations: Learning in Gather-and-Build Games}{}
    \begin{itemize}
        \item Economic environmentについて.
        \item まずは税の無い設定("free-market")で説明する.
    \end{itemize}
\end{frame}

\subsection{2.1 Notation and Preliminaries}
\begin{frame}{2.1 Notation and Preliminaries}
    \begin{itemize}
        \item Partial-observable multi-agent Markov Games(MGs): $(S, A, r, \mathscr{T}, \gamma, o, \mathscr{I})$
        \begin{itemize}
            \item $S$ : 状態空間(state space)
            \item $A$ : 行動空間(action space)
            \item $r_{i,t}$ : 報酬関数(reward function)
            \item $\mathscr{T}$ : 遷移関数(transition function)  $s_{t+1} \sim \mathscr{T}(\cdot \mid s_t, \bm{a}_t)$
            \item $\gamma$ : 割引因子(discount factor)
            \item $o_{i,t}$ : 観測(observation)
            \item time step $t = 0, 1, \dots, H$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{}{}
\begin{itemize}
    \item Agents' policy : $\pi_i(\cdot \mid o_{i,t}, h_{i,t}; \theta_i)$
    \begin{itemize}
        \item $h_{i,t}$ : hidden state（自分の私的情報と, 過去のhistory）
        \item $\theta_i$ : policyのparameter
        \item エージェント $i$ は次の最大化問題を得くpolicyを求める:
        \[
            \max_{\theta_i} \mathbb{E}_{a_i \sim \pi_i, \bm{a}_{-i} \sim \bm{\pi}_{-i}, s'\sim \mathscr{T}}\left[\sum_{t}\gamma^t r_{i,t}\right].
            \tag{1}
        \]
    \end{itemize}
    　
    \item データ効率性のため, すべてのエージェントはtrainingの間パラメータ$\theta$を共有する.
    \item 彼らの行動 $\pi_i(a_i\mid o_i, h_i; \theta)$ は, agent-specific observations $o_i$ と hidden-state $h_i$によって異なる.
\end{itemize}
\end{frame}

\begin{frame}
    \begin{center}
        \includegraphics[width=15cm]{table1.png}
    \end{center}
\end{frame}

\subsection{2.2 Environment Rules and Dynamics}
\begin{frame}{2.2 Environment Rules and Dynamics}{}
{\bf Gather-and-Build game}
\begin{itemize}
    \item 2次元のgrid ($25 \times 25$) からなる世界が舞台.
    \item エージェントはフィールドを歩き回り, 資源(石と木)を集め, それらを1つずつ使って家を建て, また資源をcoinを介してトレードする.
    \item 資源は空タイルに確率的に産み出される.
    \item エージェントは家を建てるとcoinが得られるが, 得られるcoinはagentのskillごとに異なる.
\end{itemize}
\end{frame}

\begin{frame}{}{}
{\bf Labor and Skill.}
\begin{itemize}
    \item Agentのaction(moving, gathering, trading, building)にはそれぞれlabor costが設定されている.
    \item 各timeにagentがどれか1つ行動をとると, そのlabor costがかかる.\\
    　
    \item building skill (1以上3以下)が各agentに設定されていて, 家を建てるとagentは $10 \times$ skill 分のcoinを得る.
    \item collection skill (1以上2以下)もあり, 資源を拾うとこのskill分の資源を得る\\
          （skill 1.2 の場合, 確定で1つ資源を得て, さらに確率0.2でもう1つ資源を得る）
\end{itemize}
\end{frame}

\begin{frame}{}{}
{\bf Environment Scenario.}
\begin{columns}[t]
    \begin{column}{0.6\textwidth}
        \begin{itemize}
            \item fieldは水により4つの区域に別れている（水部分は通れない）
            \item 資源は空間的に集まって発生する.
            \item 4 agents
            \item buildng skills は 1.13, 1.33, 1.65, 22.2（Pareto分布 w/ exponent $a=4$, scale $m=1$のquartilesを元に設定）
            \item 1 episodeは$H = 1000$ time stepsからなる.
        \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
        \begin{center}
            \includegraphics[width=5cm]{figure1.png}
        \end{center}
    \end{column}
\end{columns}
\end{frame}

\subsection{2.3 Using Machine Learning to Optimize Agent Behavior}
\begin{frame}{2.3 Using Machine Learning to Optimize Agent Behavior}{}
    \begin{itemize}
        \item Agentのutility function:
        \[
            u_i(x_{i,t}, l_{i,t}) = \mathrm{crra}\left(x_{i,t}^c \right) - l_{i,t},
            \ \ \ \mathrm{where}\ \ \mathrm{crra}(z) = \frac{z^{1-\eta} - 1}{1-\eta},\ \ \eta>0.
            \tag{2}
        \]
        \begin{itemize}
            \item $x_{i,t} = (x_{i,t}^w, x_{i,t}^s, x_{i,t}^c)$: $i$の保有する木・石・コイン.
            \item $l_{i,t}$: 蓄積労働量.
            \item $\eta$: エージェントのutility functionのnon-linearityをコントロールするパラメータ.
        \end{itemize}
        \item Rational economic agentは以下の最大化を行う.
        \[
            \forall i : \max_{\pi_i} \mathbb{E}_{a_i \sim \pi_i,\ \bm{a}_{-i}\sim \bm{\pi}_{-i}, s'\sim \mathscr{T}}
            \left[u_i(x_{i,0}, l_{i,0}) + \sum_{t=1}^H\gamma^t \underbrace{\left(u_i(x_{i,t}, l_{i,t})-u_i(x_{i,t-1}, l_{i,t-1})\right)}_{=r_{i,t}}\right].
            \tag{3}
        \]
    \end{itemize}
\end{frame}

\begin{frame}{}{}
{\bf Deep RL agents}
\begin{itemize}
    \item deep neural networkを用いるagent policyをmodellingする:
    \[ a_{i,t} \sim \pi(o^{\mathrm{world}}_{i,t}, o^{\mathrm{agent}}_{i,t}, o^{\mathrm{market}}_{i,t}, o^{\mathrm{tax}}_{i,t}, h_{i,t-1};\theta) \]
    \begin{itemize}
        \item $o^{\mathrm{world}}_{i,t}$: 近くの状況に関する観測.
        \item $o^{\mathrm{agent}}_{i,t}$: publicなagentの状況(資源・コイン保有)と, private agent states(skill値とlabor performed)
        \item $o^{\mathrm{market}}_{i,t}$: transfer marketの状況（bid, ask offer）
        \item $o^{\mathrm{tax}}_{i,t}$: tax rates
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{}{}
    \begin{center}
        \includegraphics[width=10.3cm]{figure3.png}
    \end{center}
\end{frame}

\begin{frame}{}{}
    {\bf Emergent Behavior of AI Agents}
\begin{columns}[t]
    \begin{column}[]{0.6\textwidth}
        \begin{center}
            \includegraphics[width=9cm]{figure4.png}
        \end{center}
    \end{column}
    \begin{column}[]{0.4\textwidth}
        \begin{itemize}
            \item 左図はno-tax下でtrain後のAI agentsの1 episodeの行動の例.
            \item low-skill agents（紺,水）は資源を集めてmarketで売ることに徹している.
            \item high-skill agent（オレンジ）はmarketで資源を買って家を建てている.
            \item 黄色は最初は家を建ててるが, のちに資源を得る方にスイッチしている.
        \end{itemize}
    \end{column}
    
\end{columns}
\end{frame}

\section{3. Machine Learning for Optimal Tax Policies}
\begin{frame}{3. Machine Learning for Optimal Tax Policies}
    \begin{itemize}
        \item 課税と再分配を行う social planner を導入する.
        \item Social plannerは, 生産性と平等性のtrade-offに直面している.
        \begin{itemize}
            \item 無課税（free-market）では生産性は最大化されるが, 不平等.
            \item 課税・再分配を行うと平等性が増すが, 生産性が落ちる.
        \end{itemize}
        　\\
        \item ここでは, free-market, US-federal, Saez framework, AI economistによるsocial plannerを試す.
    \end{itemize}
\end{frame}

\subsection{3.1 Periodic Taxes with Bracketed Schedules}
\begin{frame}{3.1 Periodic Taxes with Bracketed Schedules}{}
{\bf Income Taxes.}
    \begin{itemize}
        \item Tax periodは $M$ steps続く（実験では$M = H/10$とし, 1 episodeに10 tax periodsがあるものとする）
        \item ピリオド $p$ の税は, time step $t$ から $t + M$ までの収入 $z_i^p$ に課される.\\
                　
        \item Tax periodの初めに, social plannerはtax schedule $T(z)$ を決めて公表する.
        \begin{itemize}
            \item 各agent $i$ は, 収入 $z_i^p$ に応じて $T(z)$ を支払う.
            \item 集められた税は, 全agentに平等に分配される.
            \item よって, 分配後のagent $i$ の収入は,
            \[ \tilde{z}_i^p = z_i^p - T(z_i^p) + \frac{1}{N}\sum_{j=1}^N T\left(z_j^p\right). \tag{5}\]
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{}{}
{\bf Bracketed Tax Schedules.}
\begin{itemize}
    \item Scheme間の比較を可能にするため, tax scheduleは次のように "bracketed" されたもののみを考える.
    \item Cut-off income levels $\{m_b\}_{b = 0}^B$ s.t. $0 = m_0 \le m_1 \le \dots \le m_{B-1}\le m_B = +\infty$ が先に与えられている.
    \item Social plannerは, 各bracket $b$ に含まれる収入に対して適用されるmarginal tax rate $\tau_b \in [0,1]$を選ぶことで, tax schedule $T(\cdot)$ を決定する.
    \[ T(z) = \sum_{b = 0}^{B-1} \tau_b \cdot \left((m_{b+1}-m_{b}) \cdot 1[z > m_{b+1}] + (z - m_b)\cdot 1[m_b < z \le m_{b+1}]\right).\]
\end{itemize}
\end{frame}

\begin{frame}{3.2 Optimal Taxation}{}
{\bf Social Welfare Functions}
\begin{itemize}
    \item Social plannerの目的関数であるsocial welfare functionは, 生産性と平等性のtrade-offを組み込めるように次のように決める.
    \item エージェントのコイン保有 $\bm{x}^c = (x_1^c, \dots, x_N^c)$ に対し, equalityを次で定義:
    \[ {\bf{eq}}(\bm{x}^c) = 1 - {\bf{gini}}(\bm{x}^c)\cdot \frac{N}{N-1},\ \ \ 0 \le {\bf eq}(\bm{x}^c)\le 1.\tag{7} \]
    where
    \[ {\bf{gini}}(\bm{x}^c) = \frac{\sum_{i=1}^N\sum_{j=1}^N |x_{i}-c - x_{j}^c|}{2N \sum_{i=1}^N x_{i}^c},
    \ \ \ 0 \le {\bf gini}(\bm{x}^c) \le \frac{N-1}{N}\tag{8} \]
    \begin{itemize}
        \item ${\bf eq}$は, $1$で完全に平等(全員同じ収入), $0$で完全に不平等(1人が全コインを独占).
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{}{}
    \begin{itemize}
        \item 生産性は, 
        \[ {\bf prod}(\bm{x}^c) = \sum_{i=1}^N x_{i}^c. \tag{9} \]
        \item この ${\bf eq}$ と ${\bf prod}$ をsocial welfare functionとする.\footnote{Social welfare functionとして, weight $\omega_i \ge 0$ を用いて
        \[ {\bf swf}_t(\bm{x}_t^c, \bm{l}_t) = \sum_{i = 1}^N \omega_i \cdot u_i\left(x_{i,t}^c, l_{i,t}\right). \tag{11}\]
        を用いることも可能.}
        \[ {\bf swf}_t(\bm{x}_t^c) = {\bf eq}_t(\bm{x}_t^c) \cdot {\bf prod}_t (\bm{x}_t^c). \tag{10}\]
    \end{itemize}
\end{frame}

\begin{frame}{}{}
{\bf The Planner's Problem.}
\begin{itemize}
    \item 
\end{itemize}
\end{frame}
\end{document}